{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2\n",
    "    \n",
    "%matplotlib inline\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import nibabel as nib\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from torchir.utils import IRDataSet\n",
    "from torch.utils.data import Dataset\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "DEST_DIR = Path('./CT_output')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom dataset for loading 3D CT images\n",
    "class CTDataSet(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        self.image_paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load NIFTI image\n",
    "        image = nib.load(self.image_paths[idx]).get_fdata()\n",
    "        \n",
    "        # Convert the image data to float32 type, as PyTorch's default is float32\n",
    "        image = np.asarray(image, dtype=np.float32)\n",
    "\n",
    "        # Add channel dimension: [1, depth, height, width]\n",
    "        image = np.expand_dims(image, axis=0)\n",
    "        \n",
    "        # Apply transformations if any\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing NIFTI images\n",
    "image_dir = Path(\"E:/s4692034/thorax_resampled\")\n",
    "image_paths = [str(p) for p in image_dir.glob(\"*.nii\")]\n",
    "# print(f\"Found {len(image_paths)} images\")\n",
    "\n",
    "# TODO: how to make the network accept different matrix size\n",
    "transform = None\n",
    "\n",
    "# Create an instance of the CTDataSet\n",
    "ct_dataset = CTDataSet(image_paths=image_paths, transform=transform)\n",
    "\n",
    "# Use the IRDataSet wrapper to produce pairs for registration tasks\n",
    "registration_dataset = IRDataSet(ct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different input matrix size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the directory containing NIFTI images\n",
    "image_dir = Path(\"/30days/s4692034/Diff_matrix\")\n",
    "image_paths = [str(p) for p in image_dir.glob(\"*.nii\")]\n",
    "\n",
    "# TODO: how to make the network accept different matrix size\n",
    "transform = None\n",
    "\n",
    "# Create an instance of the CTDataSet\n",
    "ct_dataset = CTDataSet(image_paths=image_paths, transform=transform)\n",
    "\n",
    "# Use the IRDataSet wrapper to produce pairs for registration tasks\n",
    "registration_dataset = IRDataSet(ct_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_slices(volume, slice_direction=\"axial\"):\n",
    "    \"\"\"\n",
    "    Visualize 2D slices from a 3D volume.\n",
    "    \n",
    "    Parameters:\n",
    "        - volume: 3D numpy array representing the CT volume.\n",
    "        - slice_direction: Direction to slice the volume. Can be \"axial\", \"coronal\", or \"sagittal\".\n",
    "    \"\"\"\n",
    "    if slice_direction == \"axial\":\n",
    "        num_slices = volume.shape[2]\n",
    "    elif slice_direction == \"coronal\":\n",
    "        num_slices = volume.shape[1]\n",
    "    elif slice_direction == \"sagittal\":\n",
    "        num_slices = volume.shape[0]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid slice_direction. Choose from 'axial', 'coronal', or 'sagittal'.\")\n",
    "    \n",
    "    # Function to display a specific slice\n",
    "    def display_slice(slice_idx):\n",
    "        if slice_direction == \"axial\":\n",
    "            plt.imshow(volume[:, :, slice_idx], cmap=\"gray\")\n",
    "        elif slice_direction == \"coronal\":\n",
    "            plt.imshow(volume[:, slice_idx, :], cmap=\"gray\")\n",
    "        else:  # sagittal\n",
    "            plt.imshow(volume[slice_idx, :, :], cmap=\"gray\")\n",
    "        plt.axis(\"off\")\n",
    "        plt.title(f\"{slice_direction.capitalize()} Slice {slice_idx}\")\n",
    "    \n",
    "    # Use interactive slider for slice selection\n",
    "    from ipywidgets import interact\n",
    "    interact(display_slice, slice_idx=(0, num_slices-1))\n",
    "\n",
    "# Example usage:\n",
    "# Assuming 'fixed_image' and 'moving_image' are 3D numpy arrays from your dataset\n",
    "fixed_image = np.squeeze(registration_dataset[0][\"fixed\"])\n",
    "moving_image = np.squeeze(registration_dataset[0][\"moving\"])\n",
    "\n",
    "\n",
    "# Visualize slices of the fixed image\n",
    "visualize_slices(fixed_image, slice_direction=\"axial\")\n",
    "\n",
    "# Visualize slices of the moving image\n",
    "visualize_slices(moving_image, slice_direction=\"axial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Set up a random number generator\n",
    "rng = np.random.default_rng(617)\n",
    "\n",
    "# Split the dataset into training, validation, and test sets\n",
    "val_set_size = 20\n",
    "test_set_size = 100\n",
    "train_set_size = len(ct_dataset) - val_set_size - test_set_size\n",
    "\n",
    "ds_train_subset, ds_validation_subset, ds_test_subset = random_split(ct_dataset, [train_set_size, val_set_size, test_set_size], \n",
    "                                                        generator=torch.Generator().manual_seed(617))\n",
    "\n",
    "print(f'Training subset size: {len(ds_train_subset)}')\n",
    "print(f'Validation subset size: {len(ds_validation_subset)}')\n",
    "print(f'Test subset size: {len(ds_test_subset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instances of the IRDataSet for training and validation sets to produce pairs for registration tasks\n",
    "ds_train = IRDataSet(ds_train_subset)\n",
    "ds_validation = IRDataSet(ds_validation_subset)\n",
    "\n",
    "print(f'Training IR set size: {len(ds_train)}')\n",
    "print(f'Validation IR set size: {len(ds_validation)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 2\n",
    "training_batches = 50\n",
    "validation_batches = 10\n",
    "\n",
    "# RandomSampler: Samples elements randomly from a given list of indices, with replacement.\n",
    "# num_samples: ensures that we get the desired number of training samples for each epoch.\n",
    "train_sampler = torch.utils.data.RandomSampler(ds_train, replacement=True, \n",
    "                                               num_samples=training_batches*batch_size, \n",
    "                                               generator=torch.Generator().manual_seed(617))\n",
    "\n",
    "# DataLoader: Combines a dataset and a sampler, and provides an iterable over the given dataset.\n",
    "train_loader = torch.utils.data.DataLoader(ds_train, batch_size, sampler=train_sampler)\n",
    "\n",
    "# Since no sampler is provided, it will simply iterate over the dataset in its original order.\n",
    "val_loader = torch.utils.data.DataLoader(ds_validation, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchir.networks import AIRNet\n",
    "from torchir.transformers import AffineTransformer\n",
    "from torchir.metrics import NCC\n",
    "\n",
    "\n",
    "class LitAIRNet(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.airnet = AIRNet(kernels=16, ndim=3)\n",
    "        self.global_transformer = AffineTransformer(ndim=3)\n",
    "        self.metric = NCC()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = 0.001\n",
    "        optimizer = torch.optim.Adam(self.airnet.parameters(), lr=lr, amsgrad=True)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, fixed, moving):\n",
    "        # Get the transformation parameters between the fixed and moving images\n",
    "        parameters = self.airnet(fixed, moving)\n",
    "\n",
    "        # Apply this transformation to the moving image, producing the \"warped\" image.\n",
    "        warped  = self.global_transformer(parameters, fixed, moving)\n",
    "        return warped\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        warped = self(batch['fixed'], batch['moving'])\n",
    "        loss = self.metric(batch['fixed'], warped)\n",
    "        self.log('NCC/training', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        warped = self(batch['fixed'], batch['moving'])\n",
    "        loss = self.metric(batch['fixed'], warped)\n",
    "        self.log('NCC/validation', loss)\n",
    "        return loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitAIRNet()\n",
    "trainer = pl.Trainer(default_root_dir=DEST_DIR, \n",
    "                     log_every_n_steps=50,\n",
    "                     val_check_interval=50, \n",
    "                     max_epochs=100)\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(DEST_DIR / 'mnist_ir_affine.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "model = LitAIRNet.load_from_checkpoint(checkpoint_path=\"/data/home/s4692034/PythonProject/TorchIR/output/lightning_logs/version_0/checkpoints/epoch=25-step=130.ckpt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your 3D CT images have shape (500, 500, 350)\n",
    "avg_moving = np.zeros((500, 500, 350), dtype=float)\n",
    "avg_warped = np.zeros((500, 500, 350), dtype=float)\n",
    "\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "fixed = ds_test_subset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for moving in tqdm(ds_test_subset):\n",
    "        # If they are numpy arrays, convert to PyTorch tensors\n",
    "        if isinstance(fixed, np.ndarray):\n",
    "            fixed = torch.tensor(fixed)\n",
    "        if isinstance(moving, np.ndarray):\n",
    "            moving = torch.tensor(moving)\n",
    "\n",
    "        # moving = data['moving']\n",
    "        warped = model(fixed[None].cpu(), moving[None].cpu()).detach().squeeze().cpu().numpy()\n",
    "        \n",
    "        avg_moving += moving.squeeze().cpu().numpy() / len(ds_test_subset)\n",
    "        avg_warped += warped / len(ds_test_subset)\n",
    "\n",
    "# Now, for visualization, you might want to display a slice from the middle\n",
    "slice_idx = 100\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(avg_moving[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Images before registration')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(avg_warped[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Image after registration')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(fixed.squeeze().cpu().numpy()[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Fixed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert avg_warped to NIFTI image\n",
    "warped_img_nifti = nib.Nifti1Image(avg_warped, np.eye(4))\n",
    "\n",
    "# Save the image\n",
    "nib.save(warped_img_nifti, './avg_warped.nii.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nibabel as nib\n",
    "\n",
    "path = 'E:/s4692034/thorax_resampled/resampled_63464433c8_CT_cropped_normalized.nii'\n",
    "image = nib.load(path).get_fdata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "from torchir.metrics import NCC\n",
    "from torchir.dlir_framework import DLIRFramework\n",
    "from torchir.networks import DIRNet\n",
    "from torchir.transformers import BsplineTransformer\n",
    "\n",
    "class LitDLIRFramework(pl.LightningModule):\n",
    "    def __init__(self, only_last_trainable=True):\n",
    "        super().__init__()\n",
    "        self.dlir_framework = DLIRFramework(only_last_trainable=only_last_trainable)\n",
    "        self.add_stage = self.dlir_framework.add_stage\n",
    "        self.metric = NCC()\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        lr = 0.002\n",
    "        weight_decay = 0\n",
    "        optimizer = torch.optim.Adam(self.dlir_framework.parameters(), lr=lr, weight_decay=weight_decay, amsgrad=True)\n",
    "        return {'optimizer': optimizer}\n",
    "\n",
    "    def forward(self, fixed, moving):\n",
    "        warped = self.dlir_framework(fixed, moving)\n",
    "        return warped\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        warped = self(batch['fixed'], batch['moving'])\n",
    "        loss = self.metric(batch['fixed'], warped)\n",
    "        self.log('NCC/training', loss)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        warped = self(batch['fixed'], batch['moving'])\n",
    "        loss = self.metric(batch['fixed'], warped)\n",
    "        self.log('NCC/validation', loss)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LitDLIRFramework()\n",
    "model.add_stage(network=DIRNet(grid_spacing=(8, 8, 8), kernels=16, num_conv_layers=5, num_dense_layers=2, ndim=3),\n",
    "                transformer=BsplineTransformer(ndim=3, upsampling_factors=(8, 8, 8)))\n",
    "trainer = pl.Trainer(default_root_dir=DEST_DIR,\n",
    "                     log_every_n_steps=50,\n",
    "                     val_check_interval=50,\n",
    "                     max_epochs=100,\n",
    "                     strategy='ddp_spawn')\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.save_checkpoint(DEST_DIR / 'mnist_dlir_8.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your 3D CT images have shape (500, 500, 350)\n",
    "avg_moving = np.zeros((500, 500, 350), dtype=float)\n",
    "avg_warped = np.zeros((500, 500, 350), dtype=float)\n",
    "\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "fixed = ds_test_subset[0]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for moving in tqdm(ds_test_subset):\n",
    "        # If they are numpy arrays, convert to PyTorch tensors\n",
    "        if isinstance(fixed, np.ndarray):\n",
    "            fixed = torch.tensor(fixed)\n",
    "        if isinstance(moving, np.ndarray):\n",
    "            moving = torch.tensor(moving)\n",
    "\n",
    "        # moving = data['moving']\n",
    "        warped = model(fixed[None].cuda(), moving[None].cuda()).detach().squeeze().cpu().numpy()\n",
    "        \n",
    "        avg_moving += moving.squeeze().cpu().numpy() / len(ds_test_subset)\n",
    "        avg_warped += warped / len(ds_test_subset)\n",
    "\n",
    "# Now, for visualization, you might want to display a slice from the middle\n",
    "slice_idx = 50\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.imshow(avg_moving[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Images before registration')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.imshow(avg_warped[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Image after registration')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.imshow(fixed.squeeze().cpu().numpy()[:, :, slice_idx], cmap='gray')\n",
    "plt.title('Fixed Image')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(checkpoint_path):\n",
    "    # Load the model from the checkpoint\n",
    "    model = LitDLIRFramework.load_from_checkpoint(checkpoint_path=checkpoint_path)\n",
    "    \n",
    "    avg_moving = np.zeros((500, 500, 350), dtype=float)\n",
    "    avg_warped = np.zeros((500, 500, 350), dtype=float)\n",
    "\n",
    "    model = model.cuda()\n",
    "    model.eval()\n",
    "    fixed = ds_test_subset[0]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for moving in tqdm(ds_test_subset):\n",
    "            if isinstance(fixed, np.ndarray):\n",
    "                fixed = torch.tensor(fixed)\n",
    "            if isinstance(moving, np.ndarray):\n",
    "                moving = torch.tensor(moving)\n",
    "\n",
    "            warped = model(fixed[None].cuda(), moving[None].cuda()).detach().squeeze().cpu().numpy()\n",
    "            avg_moving += moving.squeeze().cpu().numpy() / len(ds_test_subset)\n",
    "            avg_warped += warped / len(ds_test_subset)\n",
    "\n",
    "    # Visualization\n",
    "    slice_idx = 50\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(avg_moving[:, :, slice_idx], cmap='gray')\n",
    "    plt.title('Images before registration')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(avg_warped[:, :, slice_idx], cmap='gray')\n",
    "    plt.title('Image after registration')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(fixed.squeeze().cpu().numpy()[:, :, slice_idx], cmap='gray')\n",
    "    plt.title('Fixed Image')\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # After training, load the model and visualize the results\n",
    "    # checkpoint_path = DEST_DIR / 'mnist_dlir_8.ckpt'\n",
    "    checkpoint_path = \"D:\\\\Temp\\\\s4692034\\\\TorchIR\\\\output\\\\lightning_logs\\\\version_44\\\\checkpoints\\\\epoch=9-step=250.ckpt\"\n",
    "    visualize(checkpoint_path)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "dd20cddef8d30d2e448169d5ce8568fb764bad8f0f7ca48b6a5d10740cf0ee8d"
  },
  "kernelspec": {
   "display_name": "Python 3.10.12 64-bit (conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
